Attention mechanisms have become a core component of neural models in Natural Language Processing over the past decade. These mechanisms not only deliver substantial performance improvements but also claim to offer insights into the models' inner workings. In this talk, we will highlight a series of contributions we have made that provided a critical perspective on the role of attention as a faithful explanation for model predictions, and sparked a larger conversation on the overarching goals of interpretability methods in NLP. We’ll contrast our methodological approaches and findings to highlight that there is no one-size-fits-all answer to the question “Is attention explanation?”. Finally, we’ll explore the role of attention as an explanation mechanism in today’s NLP landscape.