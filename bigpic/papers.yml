- abstract: The term emotion analysis in text subsumes various natural language processing
    tasks which have in common the goal to enable computers to understand emotions.
    Most popular is emotion classification in which one or multiple emotions are assigned
    to a predefined textual unit. While such setting is appropriate for identifying
    the reader's or author's emotion, emotion role labeling adds the perspective of
    mentioned entities and extracts text spans that correspond to the emotion cause.
    The underlying emotion theories agree on one important point; that an emotion
    is caused by some internal or external event and comprises several subcomponents,
    including the subjective feeling and a cognitive evaluation. We therefore argue
    that emotions and events are related in two ways. (1) Emotions are events; and
    this perspective is the fundament in natural language processing for emotion role
    labeling. (2) Emotions are caused by events; a perspective that is made explicit
    with research how to incorporate psychological appraisal theories in NLP models
    to interpret events. These two research directions, role labeling and (event-focused)
    emotion classification, have by and large been tackled separately. In this paper,
    we contextualize both perspectives and discuss open research questions.
  attributes:
    paper_type: N/A
    presentation_type: poster
    submitted_area: null
  authors:
  - dblp_id: https://dblp.org/pid/21/4183
    emails: roman.klinger@ims.uni-stuttgart.de
    first_name: Roman
    google_scholar_id: https://scholar.google.de/citations?user=1flvefwAAAAJ&hl=de
    homepage: https://www.romanklinger.de
    institution: University of Stuttgart
    last_name: Klinger
    name: Roman Klinger
    orcid: https://orcid.org/0000-0002-2014-6619
    semantic_scholar_id: https://www.semanticscholar.org/author/Roman-Klinger/66339110
    username: ~Roman_Klinger1
  file: 1.pdf
  id: 1
  openreview_id: Fd2YpI-le0N
  title: Where are We in Event-centric Emotion Analysis? Bridging Emotion Role Labeling
    and Appraisal-based Approaches
- abstract: We present our work towards building an infrastructure for documenting
    endangered languages with the focus on Uralic languages in particular. Our infrastructure
    consists of tools to write dictionaries so that entries are structured in XML
    format. These dictionaries are the foundation for rule-based NLP tools such as
    FSTs. We also work actively towards enhancing these dictionaries and tools by
    using the latest state-of-the-art neural models by generating training data through
    rules and lexica
  attributes:
    paper_type: N/A
    presentation_type: poster
    submitted_area: null
  authors:
  - dblp_id: https://dblp.org/pid/219/5607.html
    emails: sahkohamis@gmail.com
    first_name: Mika
    google_scholar_id: https://scholar.google.com/citations?user=uQ3O89IAAAAJ&hl=en
    homepage: https://mikakalevi.com
    institution: Rootroo Ltd
    last_name: Hämäläinen
    name: Mika Hämäläinen
    orcid: https://orcid.org/0000-0001-9315-1278
    semantic_scholar_id: https://www.semanticscholar.org/author/Mika-H%C3%A4m%C3%A4l%C3%A4inen/150213555
    username: ~Mika_Hämäläinen1
  - emails: jack.rueter@helsinki.fi
    first_name: Jack
    google_scholar_id: https://scholar.google.fi/citations?user=uKxNH-QAAAAJ&hl=fi
    homepage: https://researchportal.helsinki.fi/en/persons/jack-rueter
    institution: University of Helsinki
    last_name: Rueter
    name: Jack Rueter
    orcid: https://orcid.org/0000-0002-3076-7929
    username: ~Jack_Rueter1
  - emails: khldalnajjar@gmail.com
    first_name: Khalid
    google_scholar_id: https://scholar.google.com/citations?user=y4DW-KoAAAAJ&hl=en
    homepage: https://www.khalidalnajjar.com
    last_name: Alnajjar
    name: Khalid Alnajjar
    username: ~Khalid_Alnajjar1
  - emails: niko.partanen@helsinki.fi
    first_name: Niko
    google_scholar_id: https://scholar.google.com/citations?user=HnxjHJcAAAAJ
    institution: University of Helsinki
    last_name: Partanen
    middle_name: Tapio
    name: Niko Tapio Partanen
    orcid: https://orcid.org/0000-0001-8584-3880
    username: ~Niko_Tapio_Partanen1
  file: 2.pdf
  id: 2
  openreview_id: b7r3_Ht4d5x
  title: Working Towards Digital Documentation of Uralic Languages With Open-Source
    Tools and Modern NLP Methods
- abstract: 'This paper provides an overview of outstanding major research goals for
    the field of computational narrative understanding. Storytelling is an essential
    human practice, one that provides a sense of personal meaning, shared sense of
    community, and individual enjoyment. A number of research domains have increasingly
    focused on storytelling as a key mechanism for explaining human behavior. Now
    is an opportune moment to provide a vision of the contributions that computational
    narrative understanding can make towards this collective endeavor and the challenges
    facing the field. In addition to providing an overview of the elements of narrative,
    this paper outlines three major lines of inquiry: understanding the multi-modality
    of narrative; the temporal patterning of narrative (narrative "shape"); and socio-cultural
    narrative schemas, i.e. collective narratives. The paper concludes with a call
    for more inter-disciplinary working groups and deeper investment in building cross-cultural
    and multi-modal narrative datasets.'
  attributes:
    paper_type: N/A
    presentation_type: poster
    submitted_area: null
  authors:
  - dblp_id: https://dblp.org/pid/163/6259
    emails: andrew.piper@mcgill.ca
    first_name: Andrew
    google_scholar_id: https://scholar.google.com/citations?user=fCCYKkEAAAAJ&hl=en&oi=ao
    homepage: https://txtlab.org
    institution: McGill University
    last_name: Piper
    name: Andrew Piper
    orcid: https://orcid.org/0000-0001-9663-5999
    username: ~Andrew_Piper1
  file: 3.pdf
  id: 3
  openreview_id: vxGQlzZpw6l
  title: 'Computational Narrative Understanding: A Big Picture Analysis'
- abstract: I propose a paradigm for scientific progress in NLP centered around developing
    scalable, data-driven theories of linguistic structure.  The idea is to collect
    data in tightly scoped, carefully defined ways which allow for exhaustive annotation
    of behavioral phenomena of interest, and then use machine learning to construct
    explanatory theories of these phenomena which can form building blocks for intelligible
    AI systems.  After laying some conceptual groundwork, I describe several investigations
    into data-driven theories of shallow semantic structure using Question-Answer
    driven Semantic Role Labeling (QA-SRL), a schema for annotating verbal predicate-argument
    relations using highly constrained question-answer pairs.  While this only scratches
    the surface of the complex language behaviors of interest in AI, I outline principles
    for data collection and theoretical modeling which can inform future scientific
    progress.  This note summarizes and draws heavily on my PhD thesis.
  attributes:
    paper_type: N/A
    presentation_type: poster
    submitted_area: null
  authors:
  - dblp_id: https://dblp.org/pid/185/0981
    emails: julianjohnmichael@gmail.com
    first_name: Julian
    google_scholar_id: https://scholar.google.com/citations?user=9DDOHR8AAAAJ
    homepage: https://julianmichael.org
    institution: New York University
    last_name: Michael
    name: Julian Michael
    orcid: https://orcid.org/0000-0002-5358-3102
    semantic_scholar_id: https://www.semanticscholar.org/author/Julian-Michael/38614754
    username: ~Julian_Michael1
  file: 4.pdf
  id: 4
  openreview_id: F3qDvam0wPH
  title: 'The Case for Scalable, Data-Driven Theory: A Paradigm for Scientific Progress
    in NLP'
- abstract: 'This paper is a summary of the work done in my PhD thesis. Where I investigate
    the impact of bias in NLP models on the task of hate speech detection from three
    perspectives: explainability, offensive stereotyping bias, and fairness. Then,
    I discuss the main takeaways from my thesis and how they can benefit the broader
    NLP community. Finally, I discuss important future research directions. The findings
    of my thesis suggest that the bias in NLP models impacts the task of hate speech
    detection from all three perspectives. And that unless we start incorporating
    social sciences in studying bias in NLP models, we will not effectively overcome
    the current limitations of measuring and mitigating bias in NLP models.'
  attributes:
    paper_type: N/A
    presentation_type: poster
    submitted_area: null
  authors:
  - dblp_id: https://dblp.org/pid/263/5573
    emails: e.fatma.e@gmail.com
    first_name: Fatma
    google_scholar_id: https://scholar.google.com/citations?user=yLcaHaAAAAAJ&hl=en
    homepage: https://efatmae.github.io/
    last_name: Elsafoury
    name: Fatma Elsafoury
    orcid: https://orcid.org/0000-0002-9982-3511
    semantic_scholar_id: https://www.semanticscholar.org/author/Fatma-Elsafoury/117575452
    username: ~Fatma_Elsafoury1
  file: 6.pdf
  id: 6
  openreview_id: 5NWTOsh8WFd
  title: 'Thesis Distillation: Investigating The Impact of Bias in NLP Models on Hate
    Speech Detection'
- abstract: The expectation of Large Language Models (LLMs) to solve various societal
    problems has ignored the larger socio-technical frame of reference under which
    they operate. From a socio-technical perspective, LLMs are necessary to look at
    separately from other ML models as they have radically different implications
    in society never witnessed before. In this article, we ground Selbst et al.(2019)'s
    five abstraction traps -- The Framing Trap, The Portability Trap, The Formalism
    Trap, The Ripple Effect Trap and the Solutionism Trap in the context of LLMs discussing
    the problems associated with the abstraction and fairness of LLMs. Through learnings
    from previous studies and examples, we discuss each trap that LLMs fall into,
    and propose ways to address the points of LLM failure by gauging them from a socio-technical
    lens. We believe the discussions would provide a broader perspective of looking
    at LLMs through a sociotechnical lens and our recommendations could serve as baselines
    to effectively demarcate responsibilities among the various technical and social
    stakeholders and inspire future LLM research.
  attributes:
    paper_type: N/A
    presentation_type: poster
    submitted_area: null
  authors:
  - emails: kaustubhdhole@hotmail.com
    first_name: Kaustubh
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=xSGbQ3oAAAAJ&view_op=list_works&sortby=pubdate
    institution: BITS Pilani
    last_name: Dhole
    name: Kaustubh Dhole
    username: ~Kaustubh_Dhole1
  file: 7.pdf
  id: 7
  openreview_id: 7L8zCItdLn
  title: Large Language Models as SocioTechnical Systems
- abstract: 'We present a research narrative aimed at enabling language technology
    for multiple natural language generation (NLG) tasks in low-resource languages
    (LRLs). With approximately 7,000 languages spoken globally, many lack the resources
    required for model training. NLG applications for LRLs present two additional
    key challenges: (i) The training is more pronounced, and (ii) Zero-shot modeling
    is a viable research direction for scalability; however, generating zero-shot
    well-formed text in target LRLs is challenging. Addressing these concerns, this
    narrative introduces three promising research explorations that serve as a step
    toward enabling language technology for many LRLs. These approaches make effective
    use of transfer learning and limited supervision techniques for modeling. Evaluations
    were conducted mostly in the zero-shot setting, enabling scalability. This research
    narrative is an ongoing doctoral thesis.'
  attributes:
    paper_type: N/A
    presentation_type: poster
    submitted_area: null
  authors:
  - dblp_id: https://dblp.org/pid/276/5025.html
    emails: cs18resch11003@iith.ac.in
    first_name: Kaushal
    google_scholar_id: https://scholar.google.com/citations?user=eMb2l_kAAAAJ&hl=en
    homepage: https://kaushal0494.github.io/
    institution: Indian Institute of Technology Hyderabad
    last_name: Maurya
    middle_name: Kumar
    name: Kaushal Kumar Maurya
    semantic_scholar_id: https://www.semanticscholar.org/author/Kaushal-Kumar-Maurya/1576803006
    username: ~Kaushal_Kumar_Maurya1
  - dblp_id: https://dblp.org/pid/46/8779.html
    emails: maunendra@cse.iith.ac.in
    first_name: Maunendra
    google_scholar_id: https://scholar.google.co.in/citations?user=W8LJ-tEAAAAJ&hl=en
    homepage: https://www.iith.ac.in/~maunendra/
    institution: IIT Hyderabad and Indian Institute of Technology, Hyderabad,
    last_name: Desarkar
    middle_name: Sankar
    name: Maunendra Sankar Desarkar
    username: ~Maunendra_Sankar_Desarkar1
  file: 9.pdf
  id: 9
  openreview_id: nao3fU0oDBd
  title: Towards Low-resource Language Generation with Limited Supervision
- abstract: We argue that Transformers are essentially graph-to-graph models, with
    sequences just being a special case. Attention weights are functionally equivalent
    to graph edges. Our Graph-to-Graph Transformer architecture makes this ability
    explicit, by inputting graph edges into the attention weight computations and
    predicting graph edges with attention-like functions, thereby integrating explicit
    graphs into the latent graphs learned by pretrained Transformers.  Adding iterative
    graph refinement provides a joint embedding of input, output, and latent graphs,
    allowing non-autoregressive graph prediction to optimise the complete graph without
    any bespoke pipeline or decoding strategy. Empirical results show that this architecture
    achieves state-of-the-art accuracies for modelling a variety of linguistic structures,
    integrating very effectively with the latent linguistic representations learned
    by pretraining.
  attributes:
    paper_type: N/A
    presentation_type: poster
    submitted_area: null
  authors:
  - dblp_id: https://dblp.org/pid/h/JamesHenderson.html
    emails: james.henderson@idiap.ch
    first_name: James
    google_scholar_id: https://scholar.google.com/citations?user=CSib0ooAAAAJ
    homepage: http://idiap.ch/~jhenderson/
    institution: Idiap Research Institute
    last_name: Henderson
    name: James Henderson
    orcid: https://orcid.org/0000-0003-3714-4799
    semantic_scholar_id: https://www.semanticscholar.org/author/James-Henderson/144915758
    username: ~James_Henderson1
  - dblp_id: https://dblp.org/pid/250/2999
    emails: alireza.mohammadshahi@idiap.ch
    first_name: Alireza
    google_scholar_id: https://scholar.google.com/citations?user=PxvktvQAAAAJ&hl=en
    homepage: https://idiap.ch/~amohammadshahi/
    last_name: Mohammadshahi
    name: Alireza Mohammadshahi
    orcid: https://orcid.org/0000-0001-5206-7504
    semantic_scholar_id: https://www.semanticscholar.org/author/Alireza-Mohammadshahi/1387993874
    username: ~Alireza_Mohammadshahi1
  - emails: andrei.coman@idiap.ch
    first_name: Andrei
    google_scholar_id: https://scholar.google.com/citations?user=LZLaA4cAAAAJ
    homepage: https://www.idiap.ch/~acoman/
    last_name: Coman
    middle_name: Catalin
    name: Andrei Catalin Coman
    semantic_scholar_id: https://www.semanticscholar.org/author/Andrei-Catalin-Coman/52238710
    username: ~Andrei_Catalin_Coman1
  - dblp_id: https://dblp.org/pid/167/4660
    emails: lsmiculicich@gmail.com
    first_name: Lesly
    google_scholar_id: https://scholar.google.ch/citations?user=0JL8SrsAAAAJ&hl=en
    institution: Google
    last_name: Miculicich
    name: Lesly Miculicich
    semantic_scholar_id: https://www.semanticscholar.org/author/Lesly-Miculicich/1756193
    username: ~Lesly_Miculicich1
  file: 10.pdf
  id: 10
  openreview_id: sctP9orZaUmP
  title: Transformers as Graph-to-Graph Models
- abstract: 'Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs
    of a machine learning system based not on the output with the highest probability,
    but the output with the lowest risk (expected error) among multiple candidates.
    It is a simple but powerful method: for an additional cost at inference time,
    MBR provides reliable several-point improvements across metrics for a wide variety
    of tasks without any additional data or training. Despite this, MBR is not frequently
    applied in NLP works, and knowledge of the method itself is limited. We first
    provide an introduction to the method and the recent literature. We show that
    several recent methods that do not reference MBR can be written as special cases
    of MBR; this reformulation provides additional theoretical justification for the
    performance of these methods, explaining some results that were previously only
    empirical. We provide theoretical and empirical results about the effectiveness
    of various MBR variants and make concrete recommendations for the application
    of MBR in NLP models, including future directions in this area.'
  attributes:
    paper_type: N/A
    presentation_type: poster
    submitted_area: null
  authors:
  - dblp_id: https://dblp.org/pid/305/7615
    emails: abertsch72@gmail.com
    first_name: Amanda
    google_scholar_id: https://scholar.google.com/citations?user=G1Jw4CYAAAAJ
    homepage: https://www.cs.cmu.edu/~abertsch/
    institution: Carnegie Mellon University
    last_name: Bertsch
    name: Amanda Bertsch
    orcid: https://orcid.org/0000-0002-1368-1111
    semantic_scholar_id: https://www.semanticscholar.org/author/Amanda-Bertsch/2138301112
    username: ~Amanda_Bertsch1
  - emails: alexx@andrew.cmu.edu
    first_name: Alex
    last_name: Xie
    name: Alex Xie
    username: ~Alex_Xie1
  - dblp_id: https://dblp.org/pid/03/8155
    emails: gneubig@cs.cmu.edu
    first_name: Graham
    google_scholar_id: https://scholar.google.com/citations?user=wlosgkoAAAAJ
    homepage: http://phontron.com
    institution: Carnegie Mellon University
    last_name: Neubig
    name: Graham Neubig
    semantic_scholar_id: https://www.semanticscholar.org/author/Graham-Neubig/1700325
    username: ~Graham_Neubig1
  - dblp_id: https://dblp.org/pid/116/0475
    emails: mgormley@cs.cmu.edu
    first_name: Matthew
    google_scholar_id: https://scholar.google.com/citations?user=GU0SZmYAAAAJ&hl=en
    homepage: http://www.cs.cmu.edu/~mgormley/
    institution: School of Computer Science, Carnegie Mellon University and 3M
    last_name: Gormley
    middle_name: R.
    name: Matthew R. Gormley
    semantic_scholar_id: https://www.semanticscholar.org/author/Matthew-R.-Gormley/1762110
    username: ~Matthew_R._Gormley1
  file: 11.pdf
  id: 11
  openreview_id: Me8oVZmiEEaw
  title: 'It''s MBR All the Way Down: Modern Generation Techniques Through the Lens
    of Minimum Bayes Risk'
- abstract: Since the introduction of transformer-based language models in 2018, the
    current generation of natural language processing (NLP) models continues to demonstrate
    impressive capabilities on a variety of academic benchmarks and real-world applications.
    This progress is based on a simple but general pipeline which consists of pre-training
    neural language models on large quantities of text, followed by an adaptation
    step that fine-tunes the pre-trained model to perform a specific NLP task of interest.
    However, despite the impressive progress on academic benchmarks and the widespread
    deployment of pre-trained and fine-tuned language models in industry we still
    lack a fundamental understanding of how and why pre-trained and fine-tuned language
    models work as well as the individual steps of the pipeline that produce them.
    We makes several contributions towards improving our understanding of pre-trained
    and fine-tuned language models, ranging from analyzing the linguistic knowledge
    of pre-trained language models and how it is affected by fine-tuning, to a rigorous
    analysis of the fine-tuning process itself and how the choice of adaptation technique
    affects the generalization of models and thereby provide new insights about previously
    unexplained phenomena and the capabilities of pre-trained and fine-tuned language
    models.
  attributes:
    paper_type: N/A
    presentation_type: poster
    submitted_area: null
  authors:
  - dblp_id: https://dblp.org/pid/228/8380
    emails: mmosbach@lsv.uni-saarland.de
    first_name: Marius
    google_scholar_id: https://scholar.google.de/citations?user=O7RwHEkAAAAJ&hl=de&oi=ao
    homepage: https://mmarius.github.io/
    institution: Saarland University
    last_name: Mosbach
    name: Marius Mosbach
    semantic_scholar_id: https://www.semanticscholar.org/author/Marius-Mosbach/51510489
    username: ~Marius_Mosbach1
  file: 12.pdf
  id: 12
  openreview_id: wdfFjo11gIm4
  title: Analyzing Pre-trained and Fine-tuned Language Models
